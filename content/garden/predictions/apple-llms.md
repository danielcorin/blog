---
title: "✅ Apple LLMs"
draft: false
---

Originally published [here]({{< ref "logs/2023/08/01.md" >}}).


## Prediction

While not an entirely unique perspective, I believe Apple is one of the best positioned companies to take advantage of the recent improvements in language models. I expect more generic chatbots will continue to become commodities whereas Apple will build a bespoke, multi-modal assistant with access to all your personal data on device. This assistant will be able to do anything the phone can do (invoke functions/tools) as well as answer any question about your personal data (show me photos from Christmas in 2018). Let’s hope they name it something other than Siri.

Falsifiable prediction: Apple releases a personalized, multimodal model on iPhone before end of 2024.

## Outcome

2024-06-10

These features have been announced for release in the fall: https://www.apple.com/apple-intelligence/.
One thing I was wrong about: Apple will not run all inference on devices.
They are building a [private cloud](https://security.apple.com/blog/private-cloud-compute/) to which devices can offload some model inference.
There are still unanswered questions about how all this will work and when it will occur.
