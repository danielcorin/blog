---
date: "2024-06-10T21:44:51Z"
title: "2024-06-10"
draft: false
tags:
---

{{< tweet user="matthew_d_green" id="1800291897245835616" >}}

Matthew wrote an interested thread summarizing Apple's private cloud and their security approach.
Between the speed at which models are changing and their size, it's not currently practical to run things like LLM inference on iPhones, at least not for the best available models.
These models will probably always be larger or require more compute than what handheld devices are capable of.
If you want to use the best models, you'll need to offload inference to a data center.

With today's WWDC, my [prediction]({{< ref "garden/predictions/apple-llms.md" >}}) about Apple release LLMs this year proved mostly correct.
This prediction was a pretty easy one to get right but I think I got the narrative mostly correct as well.
What I didn't expect was for Apple to announce the launch of a private cloud to run model inference.
It will be interesting to see what use cases get offloaded to the cloud.
It might have to be most of them to avoid the large storage footprint of models weights on a device.
The popular, small model, [Mistral 7B](https://ollama.com/library/mistral), requires more than 4GB of model weights.
This model would take up around 6% of the 64GB of the storage on the iPhone SE, the cheapest iPhone you can buy from Apple today (with smallest storage option).
Also, this 4GB is for just _one_ model and a small one at that.
[Llama3 70B](https://ollama.com/library/llama3:70b) requires 40GB of model weights.
Storing model weights on most devices is impractical.
The iPhone 15 Pro maxes out at 1TB of storage (ðŸ¤¯), so if you have one of those, maybe you'd be willing to give away 4% of your storage to run llama3 70B on your device.
